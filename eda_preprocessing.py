# -*- coding: utf-8 -*-
"""EDA_Preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1naMOiooIvNr4uPxNIc7_tPXeMY08WuSx
"""

#

"""**EDA----Exploratory data Analysis**

 in data frame if any field is not entered with values then such values re called as missing values
 --we will fill missing values with NAN before filling it with numbers
 NAN-----> Not a number

understanding and them cleaning
Domain Knowledge---study each column
EDA--Exploratory Data Analysis
understand the data column by column  and data by data
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib .pyplot as plt

import warnings
warnings.filterwarnings('ignore')

df=pd.read_csv("data1.csv")
df.head()

df.dropna()  #drop all rows with nan

"""There are 3 rules to handle missing values
1.if missing values are <3 then drop entire row
2.if missing value are b/w 4 -50% , fill missing values using fillna()
3.if missing value are greater than > 50 then drop entire column
"""

df.isna().sum()  #return total no of nan values

#find mean of python col
pmean=df['Python'].mean()
pmean

#find mean of ML col
mlmean=df['Machine Learning'].mean()
mlmean

#fill nan values by mean
df['Python'].fillna(pmean,inplace=True)

df['Machine Learning'].fillna(mlmean,inplace=True)

df.isna().sum()

df.head()

df=pd.read_csv("cars.csv")
df.head()

#Pre-processing ---cleaning of data
#1.Check null or nan values--remove them
#2.Check junk character---remove them
#3.Remove Outliers
#Skewness and remove skweness
#Handel categorical data
#if required scale the data
#Feature Engineering

df.isnull().sum()

df.info()

df['normalized-losses'].unique()

df['horsepower'].unique()

#Whenever you have junk character convert into nan
df['normalized-losses'].replace("?",np.nan,inplace=True)

df['normalized-losses'].unique()

df['horsepower'].replace("?",np.nan,inplace=True)

#Check the column datatype--if it is not number then convert it
df['normalized-losses'].dtype

#convrt into float
df['normalized-losses']=df['normalized-losses'].astype("float")

df['normalized-losses'].dtype

df['horsepower'].dtype

df['horsepower']=df['horsepower'].astype("float")

df['horsepower'].dtype

df.info()

df.isnull() .sum()

#whenever there are null values or how do u handle missing values?
#1. if the col is numeric replace null value by mean of the column
#2 . if the column is categorical replace null values with mode of the repalce

df['normalized-losses'].fillna(df['normalized-losses'].mean(),inplace=True)

df['normalized-losses'].unique()

df['horsepower'].fillna(df['horsepower'].mean(),inplace=True)

df['horsepower'].unique()

#Alternate method to fill missing value
#Sklearn---->impute ---->SimpleImputer
from sklearn.impute import SimpleImputer

#it contains 2 argument missing value,startegy

si=SimpleImputer(missing_values=np.nan,strategy='mean')

"""We can fill missing values with different values 1.Mean---if there ids no skewness ,then fill missing values with mean

2.Median--if dataset is right or left skewed then fill missing values by median

3.Mode--if we have categorical data then fill missing values with mode
"""

df.iloc[:,[1,11]]=si.fit_transform(df.iloc[:,[1,11]])

"""

---Each row in ML called as observation/sample/instance/records

---each column is called as feature/attribute/variable/ip col ,independant


---the column which we need to predict id known as target column

---by default 70-80% data goes training and 20 to 30 %  data goes for testing



Rules before model building

1.Feature and target should be separated from dataset

2.Feature and response should be numeric in nature

3.feature and response should be proper shape"""

#separating feature and target

X=df.iloc[:,:-1] #all col except price
y=df.iloc[:,-1]  # all rows of last col

y.head()

#Find out Outliers
sns.boxplot(data=X,x=y)
plt.grid()
plt.show()

sns.boxplot(data=X,x=y,y='make')
plt.grid()
plt.show()



#removing outliers from datapoints
X[(X.make=='dodge')& (y>10000)]
X.drop(29,axis=0,inplace=True)
y.drop(29,axis=0,inplace=True)

X[(X.make=='honda')& (y>12000)]
X.drop(41,axis=0,inplace=True)
y.drop(41,axis=0,inplace=True)

X[(X.make=='isuzu')& (y>20000)]
X.drop(45,axis=0,inplace=True)
y.drop(45,axis=0,inplace=True)

X[(X.make=='mitsubishi')& (y>13000)]
X.drop([83,84],axis=0,inplace=True)
y.drop([83,84],axis=0,inplace=True)

X[(X.make=='plymouth')& (y>10000)]
X.drop(124,axis=0,inplace=True)
y.drop(124,axis=0,inplace=True)

X[(X.make=='toyota')& (y>15000)]
X.drop([172,178,179,180,181],axis=0,inplace=True)
y.drop([172,178,179,180,181],axis=0,inplace=True)

sns.boxplot(data=X,x=y,y='make')
plt.grid()
plt.show()

X.shape

"""#Skewness

it is measure of asymetry
it tell us how datapoints are distributed
3 types
1.Positive
2.negative
3.Zero

we reduce skewness by using 2 methods
1..Longvalue
2...squareroot value

"""

from scipy.stats import skew

colname=X.select_dtypes(['int64','float64']).columns
colname

#apply skew function on one column
skew(X["normalized-losses"])

#apply for loop for calculating skew for all col
for col in X[colname]:
  print(col)
  print(skew(X[col]))
  plt.figure()
  sns.distplot(X[col])
  plt.show()

"""--Before removing skewness we need to check correlation of features with target

follow 2 rules--
1)if the column is skewed but showing good correlation with target then dont remove skewness
2) if the column which is skewed has some negative values dont not skewness even though showing poor corelationship with target
"""

#removing skewness by using logvalue method

X['normalized-losses']=np.log(X['normalized-losses'])
skew(X['normalized-losses'])

#process of converting categorical data into numbers are called as encoding
'''
1.One hot encoding
2.Label encoding
3.target encoding
4.Frequency encoding
5.Binary encoding
6.Feature encoding'''

#one hot encoding----
'''
in this type each value will be conc=vrted into eigther 1 or 0
 1 represent inclusion
 0 represent exclusion'''

df['fuel-type'].unique()

X['fuel-type']

pd.get_dummies(X["fuel-type"])

from sklearn.preprocessing import OneHotEncoder

ohe=OneHotEncoder()

ohe.fit_transform(X[['fuel-type']]).toarray()

pd.get_dummies(X["make"])

ohe.fit_transform(X[['make']]).toarray()

#By using Label encoder
from sklearn.preprocessing import LabelEncoder

le=LabelEncoder()

le.fit_transform(X['fuel-type'])

#By using ordinal encoder
from sklearn.preprocessing import OrdinalEncoder

oe=OrdinalEncoder()

#paasing all categorical col
cat_col=X.select_dtypes(object).columns
cat_col

X[cat_col]=oe.fit_transform(X[cat_col])
X

#data scaling/Feature scaling
'''
it is process of bringing all the numeric quantity under one standered scale

2 ways
1.Min-max scaler
2. standrd scaler

from sklearn.preprocessing import  MinMaxScaler

mn=MinMaxScaler()

X.iloc[:,:]=mn.fit_transform(X.iloc[:,:])
X

from sklearn.preprocessing import  StandardScaler
sc=StandardScaler()
X.iloc[:,:]=sc.fit_transform(X.iloc[:,:])
X

#Feature Engineering
It is process of estracting and organizing the imp feaures from raw data such that fit into ML

X.head()

X.shape

y.shape

# spliting training ad testing data
from sklearn.model_selection import  train_test_split

"""The entire data frame will be divided into 4 parts
xtrain,xtest,ytrain,ytest
"""

xtrain,xtest,ytrain,ytest=train_test_split(X,y)

xtrain

ytrain

xtest